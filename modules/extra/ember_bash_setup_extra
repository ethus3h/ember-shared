#!/usr/bin/env bash

#set -x

# Routines that require extra packages: in addition to ember-shared and its dependencies, requires: ia, wget, rsync, sshpass, ssh, csvfix, jq, grab-site, phantomjs, ldconfig (from glibc), python, ffmpeg, youtube-dl, git, sqlite3, moreutils, imagemagick, xz, guix

iaOptionallyOfflineWrapper() {
    if [[ -z "$EMBER_IA_OFFLINE" ]]; then
        command ia "$@"
    else
        die "iaOptionallyOfflineWrapper called, but EMBER_IA_OFFLINE was set."
    fi
}

wgetOptionallyOfflineWrapper() {
    # Use the same environment variable
    if [[ -z "$EMBER_IA_OFFLINE" ]]; then
        command wget "$@"
    else
        die "wgetOptionallyOfflineWrapper called, but EMBER_IA_OFFLINE was set."
    fi
}

wget() {
    wgetOptionallyOfflineWrapper "$@"
}

ia() (
    trap 'die "A fatal error was reported on ${BASH_SOURCE[0]} line ${LINENO} in $(pwd) at $(emdate)."' ERR
    uploadCacheMetadata="false"
    if [[ "$1" == "--upload-no-cache-metadata" ]]; then
        shift
    elif [[ "$1" == "--upload-cache-metadata" ]]; then
        uploadCacheMetadata="true"
        shift
    fi
    propagateMetadataRetrieval="true"
    if [[ "$1" == "--no-propagate-metadata-retrieval" ]]; then
        # This is an internal option used to avoid recursively looking up metadata (I doubt it's otherwise useful)
        propagateMetadataRetrieval="false"
        shift
    elif [[ "$1" == "--propagate-metadata-retrieval" ]]; then
        shift
    fi
    propagateMetaXMLStored="--propagate-metaXMLStored"
    if [[ "$1" == "--no-propagate-metaXMLStored" ]]; then
        # This is an internal option used to avoid recursively looking up metadata (I doubt it's otherwise useful)
        propagateMetaXMLStored="--no-propagate-metaXMLStored"
        shift
    elif [[ "$1" == "--propagate-metaXMLStored" ]]; then
        shift
    fi
    cacheMetadata="--cache-metadata"
    if [[ "$1" == "--no-cache-metadata" ]]; then
       cacheMetadata="--no-cache-metadata"
       propagateMetaXMLStored="--no-propagate-metaXMLStored"
       shift
    elif [[ "$1" == "--cache-metadata" ]]; then
       shift
    fi

    if [[ "$1" == "upload" ]]; then
        (
            # Retry wrapper for ia to work around https://github.com/jjjake/internetarchive/issues/183
            wantsDelete="false"
            optionArgumentPattern="^--"
            remoteNameTwopartArgumentPattern="^--remote-name="
            newParams=()
            fileArgs=()
            iterCount=0
            identifier=""
            remoteNameOverride=""
            remoteNameOverrideFound="false" # Is set to true when waiting for the second half of a two-part --remote-name option
            outerSucceeded="false"
            for param in "$@"; do
                if [[ "$remoteNameOverrideFound" == "true" ]]; then
                    # The previous argument was --remote-name, so this one is the remote name to use. Because ia only allows a single file when using --remote-name, this doesn't try to handle the case of multiple --remote-name options.
                    remoteNameOverride="$param"
                    remoteNameOverrideFound="false"
                elif [[ "$param" == "--delete" ]]; then
                    wantsDelete="true"
                    continue # Don't pass the --delete argument to ia, since this wrapper should handle it instead
                elif [[ "$param" == "--remote-name" ]]; then
                    remoteNameOverrideFound="true"
                elif [[ "$param" =~ $remoteNameTwopartArgumentPattern ]]; then
                    # This is a --remote-name argument that looks like --remote-name=blah. So, remove the prefix and set the remote name.
                    remoteNameOverride="${param#--remote-name=*}"
                fi
                if ! [[ "$param" =~ $optionArgumentPattern ]]; then
                    # Skip first two arguments ("upload" and identifier). iterCount variable is needed to figure out where we are in the argument list.
                    if [[ "$iterCount" == "0" ]]; then
                        iterCount=1
                    elif [[ "$iterCount" == "1" ]]; then
                        identifier="$param"
                        iterCount=2
                    else
                        if [[ "$param" != "-" ]]; then
                            fileArgs+=("$param")
                        fi
                    fi
                fi
                newParams+=("$param")
            done
            uploadRetryInner() (
                retryCount=0
                succeeded="false"
                while [[ "$succeeded" != "true" && "$retryCount" -lt 30 ]]; do
                    retryCount=$(( retryCount + 1 ))
                    if [[ "$retryCount" -gt 1 ]]; then
                        warn "Something went wrong: retrying upload."
                        sleep 60
                    fi

                    # Use command to prevent calling this function again.
                    iaOptionallyOfflineWrapper "${newParams[@]}" && succeeded="true"
                done
                if [[ "$retryCount" -ge 30 ]]; then
                    exit 1
                fi

                retryCount=0
                succeeded="false"
                while [[ "$succeeded" != "true" && "$retryCount" -lt 5 ]]; do
                    retryCount=$(( retryCount + 1 ))
                    if [[ "$retryCount" -gt 1 ]]; then
                        warn "Something went wrong: retrying upload integrity check."
                        sleep 60
                    fi

                    # Iterate through the uploaded files and check their checksums
                    echo "Please wait: confirming upload integrity..."
                    for file in "${fileArgs[@]}"; do
                        local knownChecksum
                        knownChecksum="$(sha1sum "$file" | awk '{print $1;}')"
                        fileToQuery=""
                        if [[ -z "$remoteNameOverride" ]]; then
                            fileToQuery="$(basename "$file")"
                        else
                            fileToQuery="$remoteNameOverride"
                        fi
                        retryUntilSuccess 100 240 10 10 iasha1 --no-cache-metadata "$identifier/$fileToQuery" > /dev/null || die "Could not get checksum for uploaded file $identifier/$fileToQuery."
                        checksumLine="$(iasha1 --no-cache-metadata "$identifier/$fileToQuery")"
                        if [[ "$checksumLine" != "$knownChecksum" ]]; then
                            die "ERROR! Checksum known by Internet Archive $checksumLine does not match saved checksum known locally $knownChecksum."
                        fi
                        knownChecksumNew="$(sha1sum "$file" | awk '{print $1;}')"
                        if [[ "$checksumLine" != "$knownChecksumNew" ]]; then
                            die "ERROR! Checksum known by the Internet Archive $checksumLine does not match new checksum known locally $knownChecksumNew."
                        fi
                    done && succeeded="true"
                done
                if [[ "$retryCount" -ge 5 ]]; then
                    exit 1
                fi
                [[ "$succeeded" == "true" ]] || die "Error uploading in inner function!"
            )
            retryUntilSuccess 5 240 10 10 uploadRetryInner "$@" && outerSucceeded="true"
            [[ "$outerSucceeded" == "true" ]] || die "Error uploading!"
            if [[ "true" == "$uploadCacheMetadata" ]]; then
                # Download and save the metadata
                ia metadataStored "$identifier" > /dev/null
                ia listStored "$identifier" > /dev/null
                ia filesXMLStored "$identifier" > /dev/null
                ia metaXMLStored "$identifier" > /dev/null
            fi
            if [[ "$outerSucceeded" == "true" ]] && [[ "$wantsDelete" == "true" ]]; then
                for file in "${fileArgs[@]}"; do
                    rm "${file:?}"
                done
            fi
        )
    elif [[ "$1" == "checkeddl" ]]; then
        # Wrapper around ia download. TODO: Provide the ability to use local copies (not yet implemented)
        set -x
        (
            # iterate over all but first argument
            for param in "${@:2}"; do
                retryCount=0
                retryCheckedLocation=0
                succeeded=""
                ident=""
                ident="$(iaident "$param")"
                fileName="$(iapath "$param")"

                # Download and save the metadata?
                if [[ "--cache-metadata" == "$cacheMetadata" ]] && [[ "--no-propagate-metaXMLStored" != "$propagateMetaXMLStored" ]]; then
                    # Has not been invoked by metaXMLStored, so do it. (Otherwise, leave it to metaXMLStored to manage.)
                    ia --no-propagate-metadata-retrieval metadataStored "$ident" > /dev/null
                    ia --no-propagate-metadata-retrieval listStored "$ident" > /dev/null
                    ia --no-propagate-metadata-retrieval filesXMLStored "$ident" > /dev/null
                    ia --no-propagate-metadata-retrieval metaXMLStored "$ident" > /dev/null
                fi

                while [[ "$succeeded" != "true" && "$retryCount" -lt 30 ]]; do
                    trap 'warn "A nonfatal error was reported on ${BASH_SOURCE[0]} line ${LINENO} in $(pwd) at $(emdate). Continuing attempt."' ERR
                    succeeded=""
                    retryCount=$(( retryCount + 1 ))
                    if [[ "$retryCount" -gt 3 ]]; then
                        warn "Something went wrong: retrying download."
                        sleep 60
                    fi

                    # Now, try to get the file from various possible places.
                    if [[ "$retryCheckedLocation" == 0 ]]; then
                        backupLocation="$(crystallize-getconf BackupLocation)"
                        if [[ -d "$backupLocation/items/$ident" ]]; then
                            # A bit more stubborn rsyncing to try to get things off of flaky sshfs mount.
                            # sshfs is considered a local source, and --append-verify doesn't work for local files because it would need to read the first chunk of the file to get a checksum for comparison anyway (it would be nice if they put a note in the manpage, though).
                            retryUntilSuccess 300 1 1 1 rsync -av --partial --progress --append "$backupLocation/items/$ident" ./ &>/dev/null
                            retryCheckedLocation=1
                        else
                            # This crystal is not in the backup.
                            retryCheckedLocation=1
                            continue
                        fi
                    elif [[ "$retryCheckedLocation" == 1 ]]; then
                        # Rsync is being really slow for some reason. Maybe because the remote computer is already uploading stuff? I wouldn't think it would be that bad, though; I can ssh in (although that's a bit laggy too)
                        if false; then
                            backupLocation="$(crystallize-getconf RsyncBackupLocation)"
                            if [[ -n "$backupLocation" ]] && [[ "$backupLocation" != "exampleUser@example.hostname:'/Example/Path To Backup Directory/'" ]]; then
                                # http://web.archive.org/web/20190402151655/https://stackoverflow.com/questions/12845206/check-if-file-exists-on-remote-host-with-ssh
                                local backupLocTemp=""
                                backupLocTemp="${backupLocation#*:}"
                                backupLocTemp="${backupLocTemp#\'*}"
                                backupLocTemp="${backupLocTemp%*\'}"
                                if sshpass -p "$(crystallize-getconf RsyncBackupPassphrase)" ssh -q "${backupLocation%:*}" [[ -d \'"$backupLocTemp/items/$ident"\' ]]; then
                                    # There's a rsync location configured, and it has the desired file.
                                    sshpass -p "$(crystallize-getconf RsyncBackupPassphrase)" rsync -av --progress --append-verify --checksum "$backupLocation/items/$ident" ./ || sshpass -p "$(crystallize-getconf RsyncBackupPassphrase)" rsync -av --progress --append-verify --checksum "$backupLocation/items/$ident" ./ || sshpass -p "$(crystallize-getconf RsyncBackupPassphrase)" rsync -av --progress --append-verify --checksum "$backupLocation/items/$ident" ./ || [[ -d "$ident" ]] && retryUntilSuccess 30 1 1 1 sshpass -p "$(crystallize-getconf RsyncBackupPassphrase)" rsync -av --progress --append-verify --checksum "$backupLocation/items/$ident" ./
                                    if [[ ! -d "$ident" ]]; then
                                        retryCheckedLocation=2
                                        continue
                                    fi
                                fi
                            fi
                            retryCheckedLocation=2
                        else
                            # This crystal is not in the backup.
                            retryCheckedLocation=2
                            continue
                        fi
                    else
                        # Use command to prevent calling this function again.
                        if [[ -n "$fileName" ]]; then
                            iaOptionallyOfflineWrapper download "$ident" "$fileName" -C
                        else
                            iaOptionallyOfflineWrapper download "$ident" -C
                        fi
                    fi

                    # Now, check the downloaded files
                    if [[ -n "$fileName" ]]; then
                        file="$ident/$fileName"
                        localsha1="$(sha1sum "$file" | awk '{print $1;}')"
                        if [[ -z "$localsha1" ]]; then
                            error-notify "Got an empty sha1 for $file."
                            succeeded="false"
                            continue
                        fi
                        if [[ -f "$file" ]] && [[ "$localsha1" == "$(iasha1 "$propagateMetaXMLStored" "$cacheMetadata" "$file")" ]] && { [[ "$succeeded" == "true" ]] || [[ -z "$succeeded" ]]; }; then
                            succeeded="true"
                        else
                            succeeded="false"
                            continue
                        fi
                    else
                        # TODO: See if IA items can hold files with newlines in their names, and figure out if this breaks on that.
                        while read -r fileNameToCheck; do
                            file="$ident/$fileNameToCheck"
                            localsha1="$(sha1sum "$file" | awk '{print $1;}')"
                            if [[ -z "$localsha1" ]]; then
                                error-notify "Got an empty sha1 for $file."
                                succeeded="false"
                                continue
                            fi
                            if [[ -f "$file" ]] && [[ "$localsha1" == "$(iasha1 "$propagateMetaXMLStored" "$cacheMetadata" "$file")" ]] && { [[ "$succeeded" == "true" ]] || [[ -z "$succeeded" ]]; }; then
                                succeeded="true"
                            else
                                succeeded="false"
                                continue
                            fi
                        done < <(ia "$propagateMetaXMLStored" "$cacheMetadata" listplain "$ident")
                    fi
                done
                [[ "$succeeded" == "true" ]] || die
            done
        ) || die
        set +x
    elif [[ "$1" == "downloadAsStream" ]]; then
        # Wrapper around ia download. Provides the ability to get the result as a stream. Expects a URL rather than the ia download syntax, so it can act as a drop-in replacement for wgetting a file.
        (
            trap 'rm -r ${tempDir:?}; die "A fatal error was reported on ${BASH_SOURCE[0]} line ${LINENO} in $(pwd) at $(emdate)."' ERR
            tempDir="$(bigTempDir)"
            (
                cd "$tempDir" || die
                requestUrl="$2"
                cacheMetadataOption="--cache-metadata"
                if [[ "--no-cache-metadata" == "$requestUrl" ]]; then
                    cacheMetadataOption="--no-cache-metadata"
                    requestUrl="$3"
                elif [[ "--cache-metadata" == "$requestUrl" ]]; then
                    requestUrl="$3"
                fi
                identifier="$(iaident "$requestUrl")"
                fileName="$(iapath "$requestUrl")"
                iacontains "$propagateMetaXMLStored" "$cacheMetadataOption" "$identifier" "$fileName" || die "The Internet Archive item $identifier does not seem to contain the requested path $fileName."
                ia "$propagateMetaXMLStored" checkeddl "$requestUrl" > /dev/null || die
                [[ -f "$identifier/$fileName" ]] || die
                cat "$identifier/$fileName"
            ) || { rm -r "$tempDir"; die; }
            rm -r "$tempDir"
        ) || die
    elif [[ "$1" == "downloadHere" ]]; then
        # Wrapper around ia download. Expects a URL rather than the ia download syntax, so it can act as a drop-in replacement for wgetting a file.
        (
            requestUrl="$2"
            fileName="${requestUrl#https:\/\/archive.org\/download\/}"
            fileName="${fileName##*\/}"
            ia downloadAsStream "$requestUrl" > "$fileName" || die
        ) || die
    elif [[ "$1" == "listplain" ]]; then
        # Wrapper around ia list.
        # Test items: UntilR2V232017june27447p has commas; futuramerlin.archival.data.24nov2013 is messy
        ident="$2"
        listCommand="listStored"
        if [[ "--no-cache-metadata" == "$ident" ]] || [[ "--no-cache-metadata" == "$cacheMetadata" ]]; then
            listCommand="list"
            if [[ "--no-cache-metadata" == "$ident" ]]; then
                ident="$3"
            fi
        elif [[ "--cache-metadata" == "$ident" ]]; then
            ident="$3"
        fi
        ia "$propagateMetaXMLStored" "$listCommand" "$ident" | sed -E 's/^([^"]*,[^"]*)$/"\1"/g' | csvfix printf -fmt '%s'
    elif [[ "$1" == "listStored" ]]; then
        backupLocation="$(crystallize-getconf BackupLocation)"
        mkdir -p "$backupLocation/metadataStored/lists"
        ident="$2"
#        cacheMetadataOption="--cache-metadata"
#        if [[ "--no-cache-metadata" == "$ident" ]]; then
#            cacheMetadataOption="--no-cache-metadata"
#            ident="$3"
#        elif [[ "--cache-metadata" == "$ident" ]]; then
#            ident="$3"
#        fi
        ident="$(iaident "$ident")"
        if [[ "false" != "$propagateMetadataRetrieval" ]]; then
            # Download and save the other metadata to ensure it's all available for the item
            ia --no-propagate-metadata-retrieval metadataStored "$ident" > /dev/null
            ia --no-propagate-metadata-retrieval filesXMLStored "$ident" > /dev/null
            if [[ "--no-propagate-metaXMLStored" != "$propagateMetaXMLStored" ]]; then
                ia --no-propagate-metadata-retrieval metaXMLStored "$ident" > /dev/null
            fi
        fi
#        if [[ "--no-cache-metadata" == "$cacheMetadataOption" ]]; then
#            retryUntilSuccess 30 30 5 5 iaOptionallyOfflineWrapper list "$ident"
#        else
            path="$backupLocation/metadataStored/lists/$ident.lst"
            if [[ -e "$path" ]] && [[ "$(wc -c < "$path")" -gt 1 ]]; then
                cat "$path"
            else
                temp="$(tempFile)"
                retryUntilSuccess 30 30 5 5 iaOptionallyOfflineWrapper list "$ident" > "$temp"
                if [[ 0 != "$?" ]]; then
                    die "Could not get list for $ident: error first download."
                fi
                tempSha="$(sha512sum "$temp" | awk '{print $1;}')"
                [[ -n "$tempSha" ]] || die "Checksum empty while getting list for $ident"
                rm "$temp"
                [[ -e "$temp" ]] && die "Temporary file unexpectedly exists while getting list for $ident"
                retryUntilSuccess 30 30 5 5 iaOptionallyOfflineWrapper list "$ident" > "$temp"
                if [[ 0 != "$?" ]]; then
                    die "Could not get list for $ident: error second download."
                fi
                [[ "$(sha512sum "$temp" | awk '{print $1;}')" == "$tempSha" ]] || die "list for $ident was different when downloaded twice"
                mv "$temp" "$path"
                if [[ 0 != "$?" ]]; then
                    die "Could not get list for $ident: error moving."
                else
                    if [[ -e "$path" ]] && [[ "$(wc -c < "$path")" -gt 1 ]]; then
                        cat "$path"
                    else
                        rm "$path" || die "Could not remove path to failed retrieval of list for $ident"
                        die "Could not get list for $ident"
                    fi
                fi
            fi
#        fi
    elif [[ "$1" == "filesXMLStored" ]]; then
        backupLocation="$(crystallize-getconf BackupLocation)"
        mkdir -p "$backupLocation/metadataStored/filesXMLs"
        ident="$(iaident "$2")"
        if [[ "false" != "$propagateMetadataRetrieval" ]]; then
            # Download and save the other metadata to ensure it's all available for the item
            ia --no-propagate-metadata-retrieval metadataStored "$ident" > /dev/null
            ia --no-propagate-metadata-retrieval listStored "$ident" > /dev/null
            if [[ "--no-propagate-metaXMLStored" != "$propagateMetaXMLStored" ]]; then
                ia --no-propagate-metadata-retrieval metaXMLStored "$ident" > /dev/null
            fi
        fi
        path="$backupLocation/metadataStored/filesXMLs/$ident"_files.xml
        if [[ -e "$path" ]] && [[ "$(wc -c < "$path")" -gt 1 ]]; then
            cat "$path"
        else
            temp="$(tempFile)"
            retryUntilSuccess 30 30 5 5 wgetOptionallyOfflineWrapper -qO - "https://archive.org/download/$ident/$ident"_files.xml > "$temp"
            if [[ 0 != "$?" ]]; then
                die "Could not get files XML for $ident: error first download."
            fi
            tempSha="$(sha512sum "$temp" | awk '{print $1;}')"
            [[ -n "$tempSha" ]] || die "Checksum empty while getting files XML for $ident"
            rm "$temp"
            [[ -e "$temp" ]] && die "Temporary file unexpectedly exists while getting files XML for $ident"
            retryUntilSuccess 30 30 5 5 wgetOptionallyOfflineWrapper -qO - "https://archive.org/download/$ident/$ident"_files.xml > "$temp"
            if [[ 0 != "$?" ]]; then
                die "Could not get files XML for $ident: error second download."
            fi
            [[ "$(sha512sum "$temp" | awk '{print $1;}')" == "$tempSha" ]] || die "files XML for $ident was different when downloaded twice"
            mv "$temp" "$path"
            if [[ 0 != "$?" ]]; then
                die "Could not get files XML for $ident: error moving."
            else
                if [[ -e "$path" ]] && [[ "$(wc -c < "$path")" -gt 1 ]]; then
                    cat "$path"
                else
                    rm "$path" || die "Could not remove path to failed retrieval of files XML for $ident"
                    die "Could not get files XML for $ident"
                fi
            fi
        fi
    elif [[ "$1" == "metaXMLStored" ]]; then
        backupLocation="$(crystallize-getconf BackupLocation)"
        mkdir -p "$backupLocation/metadataStored/metaXMLs"
        ident="$(iaident "$2")"
        if [[ "false" != "$propagateMetadataRetrieval" ]]; then
            # Download and save the other metadata to ensure it's all available for the item
            ia --no-propagate-metadata-retrieval metadataStored "$ident" > /dev/null
            ia --no-propagate-metadata-retrieval listStored "$ident" > /dev/null
            ia --no-propagate-metadata-retrieval filesXMLStored "$ident" > /dev/null
        fi
        path="$backupLocation/metadataStored/metaXMLs/$ident"_meta.xml
        if [[ -e "$path" ]] && [[ "$(wc -c < "$path")" -gt 1 ]]; then
            cat "$path"
        else
            temp="$(tempFile)"
            retryUntilSuccess 30 30 5 5 wgetOptionallyOfflineWrapper -qO - "https://archive.org/download/$ident/$ident"_meta.xml > "$temp"
            if [[ 0 != "$?" ]]; then
                die "Could not get meta XML for $ident: error first download."
            fi
            tempSha="$(sha512sum "$temp" | awk '{print $1;}')"
            [[ -n "$tempSha" ]] || die "Checksum empty while getting meta XML for $ident"
            rm "$temp"
            [[ -e "$temp" ]] && die "Temporary file unexpectedly exists while getting meta XML for $ident"
            ia --no-propagate-metaXMLStored downloadAsStream "https://archive.org/download/$ident/$ident"_meta.xml > "$temp"
            if [[ 0 != "$?" ]]; then
                die "Could not get meta XML for $ident: error second download."
            fi
            [[ "$(sha512sum "$temp" | awk '{print $1;}')" == "$tempSha" ]] || die "meta XML for $ident was different when downloaded twice"
            mv "$temp" "$path"
            if [[ 0 != "$?" ]]; then
                die "Could not get meta XML for $ident: error moving."
            else
                if [[ -e "$path" ]] && [[ "$(wc -c < "$path")" -gt 1 ]]; then
                    cat "$path"
                else
                    rm "$path" || die "Could not remove path to failed retrieval of meta XML for $ident"
                    die "Could not get meta XML for $ident"
                fi
            fi
        fi
    elif [[ "$1" == "metadataStored" ]]; then
        # ia metadata seems to be nondeterministic, so wrap it with retry and hopefully when it matches, the result will be sensible
        propagateMetadataRetrievalArg="--propagate-metadata-retrieval"
        [[ "false" == "$propagateMetadataRetrieval" ]] && propagateMetadataRetrievalArg="--no-propagate-metadata-retrieval"
        retryUntilSuccess 30 30 5 5 ia "$propagateMetadataRetrievalArg" "$propagateMetaXMLStored" metadataStoredInner "${@:2}"
    elif [[ "$1" == "metadataStoredInner" ]]; then
        backupLocation="$(crystallize-getconf BackupLocation)"
        mkdir -p "$backupLocation/metadataStored/metadata"
        ident="$(iaident "$2")"
        if [[ "false" != "$propagateMetadataRetrieval" ]]; then
            # Download and save the other metadata to ensure it's all available for the item
            ia --no-propagate-metadata-retrieval listStored "$ident" > /dev/null
            ia --no-propagate-metadata-retrieval filesXMLStored "$ident" > /dev/null
            if [[ "--no-propagate-metaXMLStored" != "$propagateMetaXMLStored" ]]; then
                ia --no-propagate-metadata-retrieval metaXMLStored "$ident" > /dev/null
            fi
        fi
        path="$backupLocation/metadataStored/metadata/$ident.json"
        if [[ -e "$path" ]] && [[ "$(wc -c < "$path")" -gt 3 ]]; then
            cat "$path"
        else
            temp="$(tempFile)"
            retryUntilSuccess 30 30 5 5 iaOptionallyOfflineWrapper metadata "$ident" > "$temp"
            if [[ 0 != "$?" ]]; then
                die "Could not get metadata for $ident: error first download."
            fi
            tempSha="$(sha512sum "$temp" | awk '{print $1;}')"
            [[ -n "$tempSha" ]] || die "Checksum empty while getting metadata for $ident"
            rm "$temp"
            [[ -e "$temp" ]] && die "Temporary file unexpectedly exists while getting metadata for $ident"
            retryUntilSuccess 30 30 5 5 iaOptionallyOfflineWrapper metadata "$ident" > "$temp"
            if [[ 0 != "$?" ]]; then
                die "Could not get metadata for $ident: error second download."
            fi
            [[ "$(sha512sum "$temp" | awk '{print $1;}')" == "$tempSha" ]] || die "metadata for $ident was different when downloaded twice"
            mv "$temp" "$path"
            if [[ 0 != "$?" ]]; then
                die "Could not get metadata for $ident: error moving."
            else
                if [[ -e "$path" ]] && [[ "$(wc -c < "$path")" -gt 3 ]]; then
                    cat "$path"
                else
                    rm "$path" || die "Could not remove path to failed retrieval of metadata for $ident"
                    die "Could not get metadata for $ident"
                fi
            fi
        fi
    elif [[ "$1" == "metadata" ]]; then
        retryUntilSuccess 30 30 5 5 iaOptionallyOfflineWrapper "$@" || die
    else
        iaOptionallyOfflineWrapper "$@" || die
    fi
)

iacontains() (
    propagateMetaXMLStored="--propagate-metaXMLStored"
    if [[ "$1" == "--no-propagate-metaXMLStored" ]]; then
        # This is an internal option used to avoid recursively looking up metadata (I doubt it's otherwise useful)
        propagateMetaXMLStored="--no-propagate-metaXMLStored"
        shift
    elif [[ "$1" == "--propagate-metaXMLStored" ]]; then
        shift
    fi

    cacheMetadataOption="--cache-metadata"
    if [[ "--no-cache-metadata" == "$1" ]]; then
        cacheMetadataOption="--no-cache-metadata"
        shift
    elif [[ "--cache-metadata" == "$1" ]]; then
        shift
    fi
    ident="$(iaident "$1")"
    desiredFile="$2"
    if hasWebProtocol "$desiredFile"; then
        # This is a URL to check, not a path, so get the path from it
        desiredFile="$(iapath "$desiredFile")"
    fi
    foundDesiredFile="false"
    # Won't work if the file name contains newline
    while IFS= read -r fileNameToCheck; do
        if [[ "$desiredFile" == "$fileNameToCheck" ]]; then
            foundDesiredFile="true"
        fi
    done < <(ia "$propagateMetaXMLStored" listplain "$cacheMetadataOption" "$ident")
    if [[ "true" == "$foundDesiredFile" ]]; then
        return 0
    fi
    return 1
)

iasha1() (
    propagateMetaXMLStored="--propagate-metaXMLStored"
    if [[ "$1" == "--no-propagate-metaXMLStored" ]]; then
        # This is an internal option used to avoid recursively looking up metadata (I doubt it's otherwise useful)
        propagateMetaXMLStored="--no-propagate-metaXMLStored"
        shift
    elif [[ "$1" == "--propagate-metaXMLStored" ]]; then
        shift
    fi

    metadataCommand="metadataStored"
    if [[ "--no-cache-metadata" == "$1" ]]; then
        # This is for verifying freshly created items. Since the filesXML won't be downloaded in that case, don't bother overriding the filesXMLStored command.
        metadataCommand="metadata"
        shift
    elif [[ "--cache-metadata" == "$1" ]]; then
        shift
    fi
    local field="sha1"
    local result
    local iapathEscaped
    if [[ "$(iapath "$1")" == "$(iaident "$1")"_files.xml ]]; then
        # TODO: Decide whether this is the right way to go about this. Right now, because item files XMLs do not have checksums available for them from the Internet Archive (since the XML files are where the checksums are kept), iasha1 will retrieve it and calculate the checksum dynamically. This could be suboptimal (or even harmful) since there is no guarantee that the returned value is correct; this behavior may change in later versions. Also, check whether it works with on-the-fly derivative files, which may break when checked using this, e.g. when using crystallize-backup, because of the same issue of not having checksums listed in the XML (I don't know if they are).
        if [[ "metadata" == "$metadataCommand" ]]; then
            ident="$(iaident "$1")"
            result="$(wgetOptionallyOfflineWrapper -qO - "https://archive.org/download/$ident/$ident"_files.xml | sha1sum | awk '{print $1;}')"
        else
            result="$(ia "$propagateMetaXMLStored" filesXMLStored "$1" | sha1sum | awk '{print $1;}')"
        fi
    else
        iapathEscaped="$(iapath "$1")"
        iapathEscaped="${iapathEscaped//\"/\\\"}"
        result="$(ia "$propagateMetaXMLStored" "$metadataCommand" "$(iaident "$1")" | { jq -r '.files[] | select(.name=="'"$iapathEscaped"'").'"$field" 2>/dev/null; };)"
    fi
    # jq seems to exit 0 on at least some errors
    [[ -z "$result" ]] && exit 1
    print "$result"
)

iamd5() (
    local field="md5"
    local result
    local iapathEscaped
    if [[ "$(iapath "$1")" == "$(iaident "$1")"_files.xml ]]; then
        # TODO: Decide whether this is the right way to go about this. Right now, because item files XMLs do not have checksums available for them from the Internet Archive (since the XML files are where the checksums are kept), iamd5 will retrieve it and calculate the checksum dynamically. This could be suboptimal (or even harmful) since there is no guarantee that the returned value is correct; this behavior may change in later versions. Also, check whether it works with on-the-fly derivative files, which may break when checked using this, e.g. when using crystallize-backup, because of the same issue of not having checksums listed in the XML (I don't know if they are).
        result="$(ia filesXMLStored "$1" | md5sum | awk '{print $1;}')"
    else
        iapathEscaped="$(iapath "$1")"
        iapathEscaped="${iapathEscaped//\"/\\\"}"
        result="$(ia metadataStored "$(iaident "$1")" | { jq -r '.files[] | select(.name=="'"$iapathEscaped"'").'"$field" 2>/dev/null; };)"
    fi
    # jq seems to exit 0 on at least some errors
    [[ -z "$result" ]] && exit 1
    print "$result"
)

getGrabSitePath() {
    if command grab-site &> /dev/null; then
        command grab-site
    elif [[ -f "$HOME/gs-venv/bin/grab-site" ]]; then
        echo "$HOME/gs-venv/bin/grab-site"
    elif [[ -f "$HOME/gs-venv/gs-venv/bin/grab-site" ]]; then
        echo "$HOME/gs-venv/gs-venv/bin/grab-site"
    else
        return 1
    fi
}

getPhantomjsPath() {
    if command phantomjs &> /dev/null; then
        command phantomjs
    elif [[ -f "$HOME/phantomjs-1.9.8-linux-x86_64/bin/phantomjs" ]]; then
        echo "$HOME/phantomjs-1.9.8-linux-x86_64/bin/phantomjs"
    elif [[ -f "/phantomjs-1.9.8-linux-x86_64/bin/phantomjs" ]]; then
        echo "/phantomjs-1.9.8-linux-x86_64/bin/phantomjs"
    elif [[ -f "$HOME/bin/phantomjs" ]]; then
        echo "$HOME/bin/phantomjs"
    else
        return 1
    fi
}

grab-site() {
    "$(getGrabSitePath)" "$@"
}

phantomjs() {
    "$(getPhantomjsPath)" "$@"
}

a() (
    aq --add-wpull-args '--tries=1024' "$@"
)

o() {
    aq --add-wpull-args '--tries=1024' --1 "$@"
}

aq() (
    cd "$(getGrabSiteGrabLocation)" || die "Could not cd to grab-site grab location!"

    local addWpullArgs=''
    if [[ "$1" == "--add-wpull-args" ]]; then
        shift
        addWpullArgs=" $1"
        shift
    fi

    local phantomjsArgs=''
    if getPhantomjsPath &> /dev/null; then
        phantomjsArgs='\ --phantomjs-scroll=50000\ --phantomjs-exe='"$(getPhantomjsPath)"
    fi

    # FIXME: Add \ --retry-connrefused\ --retry-dns-error
    # See https://github.com/ludios/grab-site/issues/129
    grab-site --no-dupespotter --concurrency=3 --wpull-args=--read-timeout=3600\ --connect-timeout=20\ --dns-timeout=20\ --max-redirect=128"$phantomjsArgs"\ --content-on-error"$addWpullArgs" "$@"
)

oq() {
    aq --1 "$@"
}

ap() {
    aq --add-wpull-args '--tries=1024 --phantomjs' "$@"
}

op() {
    aq --add-wpull-args '--tries=1024 --phantomjs' --1 "$@"
}

getGrabSiteWarcLocation() {
    die "Unimplemented!" # TODO: Add optional Warcdealer support to the grab-site wrappers here.

    # TODO
    #a() { cd /home/grabbot/grabs/ && grab-site --no-dupespotter --concurrency=5 --wpull-args=--warc-move=/home/grabbot/warcdealer/\ --read-timeout=3600\ --connect-timeout=20\ --dns-timeout=20\ --retry-connrefused\ --retry-dns-error\ --max-redirect=128\ --phantomjs-scroll=50000\ --phantomjs-exe=/phantomjs-1.9.8-linux-x86_64/bin/phantomjs\ --content-on-error\ --tries=1024 "$@"; }
    #o() { cd /home/grabbot/grabs/ && grab-site --no-dupespotter --concurrency=5 --wpull-args=--warc-move=/home/grabbot/warcdealer/\ --read-timeout=3600\ --connect-timeout=20\ --dns-timeout=20\ --retry-connrefused\ --retry-dns-error\ --max-redirect=128\ --phantomjs-scroll=50000\ --phantomjs-exe=/phantomjs-1.9.8-linux-x86_64/bin/phantomjs\ --content-on-error\ --tries=1024 --1 "$@"; }
    #aq() { cd /home/grabbot/grabs/ && grab-site --no-dupespotter --concurrency=5 --wpull-args=--warc-move=/home/grabbot/warcdealer/\ --read-timeout=3600\ --connect-timeout=20\ --dns-timeout=20\ --retry-connrefused\ --retry-dns-error\ --max-redirect=128\ --phantomjs-scroll=50000\ --phantomjs-exe=/phantomjs-1.9.8-linux-x86_64/bin/phantomjs\ --content-on-error "$@"; }
    #oq() { cd /home/grabbot/grabs/ && grab-site --no-dupespotter --concurrency=5 --wpull-args=--warc-move=/home/grabbot/warcdealer/\ --read-timeout=3600\ --connect-timeout=20\ --dns-timeout=20\ --retry-connrefused\ --retry-dns-error\ --max-redirect=128\ --phantomjs-scroll=50000\ --phantomjs-exe=/phantomjs-1.9.8-linux-x86_64/bin/phantomjs\ --content-on-error --1 "$@"; }
    #ap() { cd /home/grabbot/grabs/ && grab-site --no-dupespotter --concurrency=5 --wpull-args=--warc-move=/home/grabbot/warcdealer/\ --read-timeout=3600\ --connect-timeout=20\ --dns-timeout=20\ --retry-connrefused\ --retry-dns-error\ --max-redirect=128\ --phantomjs-scroll=50000\ --phantomjs-exe=/phantomjs-1.9.8-linux-x86_64/bin/phantomjs\ --content-on-error\ --tries=1024\ --phantomjs "$@"; }
    #op() { cd /home/grabbot/grabs/ && grab-site --no-dupespotter --concurrency=5 --wpull-args=--warc-move=/home/grabbot/warcdealer/\ --read-timeout=3600\ --connect-timeout=20\ --dns-timeout=20\ --retry-connrefused\ --retry-dns-error\ --max-redirect=128\ --phantomjs-scroll=50000\ --phantomjs-exe=/phantomjs-1.9.8-linux-x86_64/bin/phantomjs\ --content-on-error\ --tries=1024\ --phantomjs --1 "$@"; }

}

findso() {
    # based on https://unix.stackexchange.com/questions/22178/which-equivalent-for-shared-libraries
    if [[ "$1" == "--plain" ]]; then
        soname="$2"
    else
        soname="lib${1}.so"
    fi
    local ldpath="$LD_LIBRARY_PATH"
    local found="false"
    if [[ -z "$ldpath" ]]; then
        ldpath="$(</etc/ld.so.conf)"
    else
        local IFS=:
    fi
    for libdir in $ldpath; do
        if [[ -f "$libdir/$soname" ]]; then
            echo "$libdir/$soname"
            found="true"
            break
        fi
    done
    if [[ "$found" == "true" ]]; then
        return 0
    fi
    /sbin/ldconfig -p | awk -v needle="$1" '$1 == needle {sub(/.* => /, ""); print}'
}

flacToWaveform() (
    sourceFile="$1"
    destinationFile="$sourceFile.png"
    if [[ -e "$destinationFile" ]] || [[ -h "$destinationFile" ]]; then
        die "The requested destination file, $destinationFile, already exists."
    fi
    tempWorkDirectory="$(tempDir)"
    tempWavFile="$tempWorkDirectory/$sourceFile.wav"
    tempPngFile="$tempWorkDirectory/$sourceFile.png"
    resPngFile="$tempWorkDirectory/res-$sourceFile.png"
    ffmpeg -v 0 -analyzeduration 900000000000 -probesize 200M  -threads 2 -i "$sourceFile" -ac 1 -ar 44100 -y "$tempWavFile" || die
    (
        cd "$tempWorkDirectory" || die
        wav2png.py "$tempWavFile" > /dev/null 2>&1 || die
    )
    convert -background black "$tempPngFile" -gravity center -extent 800x200 -background black -fuzz 50% -transparent white "$resPngFile" || die
    mv "$resPngFile" "$destinationFile" || die
    rm -r "$tempWorkDirectory"
)

bulkFlacToWaveform() {
    while [[ -n "$1" ]]; do
        flacToWaveform "$(basename "$1")"
        shift
    done
}

ytdl-quick() {
    local logFile
    logFile="ytdl-quicked-$(date-uuid)".log
    {
        youtube-dl --version
        ember_bash_setup --version
        youtube-dl -i --geo-bypass --write-description --write-info-json --write-annotations --write-thumbnail --write-all-thumbnails --write-sub --write-auto-sub --all-subs -k "$@"
    } 2>&1 | tee -a "$logFile"
}

ytdl-quick-audio() {
    ytdl-quick -f bestaudio "$@"
}

nomerge-ytdl-quick-audio() {
    local logFile
    logFile="ytdl-quicked-$(date-uuid)".log
    {
        youtube-dl --version
        ember_bash_setup --version
        youtube-dl -i --geo-bypass --write-description --write-info-json --write-annotations --write-thumbnail --write-all-thumbnails --write-sub --write-auto-sub --all-subs -k -f bestaudio/best -o '%(title)s-%(id)s.f%(format_id)s.%(ext)s' "$@"
    } 2>&1 | tee -a "$logFile"
}

nomerge-ytdl-quick-video() {
    local logFile
    logFile="ytdl-quicked-$(date-uuid)".log
    {
        youtube-dl --version
        ember_bash_setup --version
        youtube-dl -i --geo-bypass --write-description --write-info-json --write-annotations --write-thumbnail --write-all-thumbnails --write-sub --write-auto-sub --all-subs -k -f bestvideo/best -o '%(title)s-%(id)s.f%(format_id)s.%(ext)s' "$@"
    } 2>&1 | tee -a "$logFile"
}

# na- ones don't try to download annotations, for getting around videos with 404 error on annotation URL.
na-ytdl-quick() {
    local logFile
    logFile="ytdl-quick-naed-$(date-uuid)".log
    {
        youtube-dl --version
        ember_bash_setup --version
        youtube-dl -i --geo-bypass --write-description --write-info-json --write-thumbnail --write-all-thumbnails --write-sub --write-auto-sub --all-subs -k "$@"
    } 2>&1 | tee -a "$logFile"
}

na-ytdl-quick-audio() {
    na-ytdl-quick -f bestaudio "$@"
}

git-sync() {
    repo="$1"
    repoName="$(basename "$repo")"
    repoName="${repoName%.*}"
    local logFile
    logFile="git-synced-$(date-uuid)".log
    {
        echo "$repo"
        ember_bash_setup --version
        if [[ -n "$repoName" ]]; then
            if [[ -e "$repoName.git" ]]; then
                [[ -d "$repoName.git" ]] || die "$(pwd)/$repoName.git does not seem to be a directory."
                (
                    cd "$repoName.git" || die "Could not cd to repository directory $(pwd)/$repoName.git."
                    echo "Updating clone of the \"$repoName\" git repository..."
                    retryUntilSuccess 10 git fetch --all || die
                    git fsck || die "Git fsck of $repo failed"
                )
            else
                retryUntilSuccess 10 git clone --mirror "$repo" || die
                (
                    cd "$repoName.git" || die "Could not cd to repository directory $(pwd)/$repoName.git."
                    git fsck || die "Git fsck of newly cloned repo $repo failed"
                )
            fi
        fi
        echo "Successfully retrieved $repo."
    } 2>&1 | tee -a "$logFile"
}

git-bulk-sync() {
    while read -r repo; do
        git-sync "$repo"
    done
}

firefox-places-dump() {
    # help from https://gist.github.com/kirang89/6541591
    local firefoxProfile
    if [[ -n "$1" ]]; then
        firefoxProfile="$1"
    else
        # shellcheck disable=SC2119
        firefoxProfile="$(cd ~ || die "Could not cd to home folder"; get-default-firefox-profile)"
    fi
    sqlite3 "$firefoxProfile/places.sqlite" '.sch' > firefox-places-dump.schema.sql-nltc
    sqlite3 "$firefoxProfile/places.sqlite" '.dump' > firefox-places-dump.sql-nltc
    cp "$firefoxProfile/sessionstore.js" firefox-places-dump.sessionstore.js-nltc
    # Put newline every other character, to make it easier on git storage
    newline-terminated-characters-from-text < firefox-places-dump.schema.sql-nltc | sponge firefox-places-dump.schema.sql-nltc
    newline-terminated-characters-from-text < firefox-places-dump.sql-nltc | sponge firefox-places-dump.sql-nltc
    newline-terminated-characters-from-text < firefox-places-dump.sessionstore.js-nltc | sponge firefox-places-dump.sessionstore.js-nltc
}

firefox-places-build() {
    newline-terminated-characters-to-text < firefox-places-dump.sessionstore.js-nltc > sessionstore.js
    newline-terminated-characters-to-text < firefox-places-dump.sql-nltc > firefox-places-dump.tmp.sql
    sqlite3 places.sqlite < firefox-places-dump.tmp.sql
    rm firefox-places-dump.tmp.sql
}

bigSponge() (
    trap 'error-notify "An error was reported in the sponge wrapper with the arguments $* on ${BASH_SOURCE[0]} line ${LINENO} in $(pwd) at $(emdate)."' ERR
    trap - EXIT
    TMPDIR="$(getBigTempLocation)"
    export TMPDIR
    command sponge "$@"
)

checkedxz() (
    checkedxzErr() {
        rm -f "$output"
        rm -f "${streamingInput:?}"
    }
    trap 'checkedxzErr; die "A fatal error was reported on ${BASH_SOURCE[0]} line ${LINENO} in $(pwd) at $(emdate)."' ERR
    trap - EXIT
    local quiet="false"
    local requestWaitAfter="false"
    if [[ "$1" == "--quiet" ]]; then
        quiet="true"
        pv() {
            cat
        }
        shift
    elif [[ "$1" == "--request-wait-after" ]]; then
        requestWaitAfter="true"
        shift
    fi
    local input="$1"
    local useStdout=""
    if [[ -n "$input" ]]; then
        shift
        if [[ "$1" == "--stdout" ]]; then
            useStdout="true"
            shift
        fi
    else
        useStdout="true"
    fi

    local output
    output="$(bigTempFile)"
    local fileSize
    local execMode=""
    [[ "$quiet" != "true" ]] && echo "Packing..." >&2
    if [[ -z "$input" ]] || [[ "$input" == "-" ]]; then
        execMode="stdin"
        if [[ "$input" == "-" ]]; then
            useStdout="true"
        fi
        input="$(bigTempFile)"
        streamingInput="$(bigTempFile)"
        cat > "${input:?}"
        fileSize="$(disk-size-in-bytes "$input")"
        inputCsum="$(sha512sum "$input" | awk '{print $1;}')"
    else
        execMode="file"
        fileSize="$(disk-size-in-bytes "$input")"
        inputCsum="$(sha512sum "$input" | awk '{print $1;}')"
    fi
    checkedxzInner() (
        if [[ "$execMode" == "stdin" ]]; then
            pv -tparbIfei 0.1 -s "$fileSize" < "$input" | xz "$@" --stdout - > "$output"
        else
            xz -k "$@" --stdout "$input" | pv -tparbIfei 0.1 -s "$fileSize" > "$output"
        fi
        fileSize="$(disk-size-in-bytes "$output")"
        [[ "$quiet" != "true" ]] && echo "Checking..." >&2
        testOutputCsum="$(pv -tparbIfei 0.1 -s "$fileSize" < "$output" | unxz --stdout - | sha512sum | awk '{print $1;}')"
        [[ "$requestWaitAfter" == "true" ]] && echo "Please wait..." >&2
        assert equals "$inputCsum" "$testOutputCsum" || die
    )
    retryUntilSuccess 100 checkedxzInner "$@" || { [[ "$execMode" == "stdin" ]] && rm -f "${input:?}"; die; }
    if [[ "$useStdout" == "true" ]]; then
        cat "$output"
        rm "$output"
    else
        mv "$output" "$input.xz"
    fi
    if { ! contains "-k" "$@"; } || [[ "$execMode" == "stdin" ]]; then
        rm "${input:?}"
    fi
    if [[ -n "$streamingInput" ]]; then
        rm "$streamingInput"
    fi
)

updateGuixRepo() (
    workDir="$(crystallize-getconf WorkDirectory)"
    mkdir -p "$workDir"/ember-auto-build
    cd "$workDir"/ember-auto-build || die
    repository="guix"
    if [[ -e "$repository" ]]; then
        pushd "$repository" || die
        git pull
        popd || die
    else
        git clone "https://git.savannah.gnu.org/git/guix.git"
    fi
)

testGuixPackage() (
    if [[ -n "$1" ]]; then
        pn="$1"
    else
        pn="$(<package-name)"
    fi
    updateGuixRepo
    cd "$(crystallize-getconf WorkDirectory)"/ember-auto-build/guix || die
    echo
    echo './pre-inst-env guix environment --container --ad-hoc -e '\''(list (@ (gnu) %base-packages) (@ (gnu packages dce) ember-shared-core))'\''' # how to get the environment to put ember_bash_setup_core in the PATH? It's not in the container. Running ./pre-inst-env guix install ember-shared-core surprisingly seems to link it in /home/kyan/.guix-profile/usr/bin.
    echo
    printf '%b' '\033[1;31m'
    echo 'To build, run:'
    echo './pre-inst-env guix build '"$pn"'; exit'
    printf '%b' '\033[0m'
    echo
    guix environment guix || true
)

generateGuixPackage() (
    eogup-single
    git push
    buildDir="$(crystallize-getconf WorkDirectory)"
    repository="$(<package-name)"
    if [[ -z "$repository" ]]; then
        die "Please make sure you are in a repository directory."
    fi
    if [[ -e "version" ]]; then
        version="$(<version)"
    else
        version="0"
    fi
    if [[ -n "$1" ]]; then
        pn="$1"
    else
        pn="$repository"
    fi
    emberPath="$(ember-getconf EmberLibrary)"
    packagesDir="$emberPath/ember-shared/data/packages/"
    cp "$packagesDir"/dce-in.scm ~/ember-auto-build-temp/dce.scm
    echo "$pn" >> "$packagesDir"/packages.lst
    cat "$packagesDir"/packages.lst | removeDuplicateLines | sponge "$packagesDir"/packages.lst
    mkdir -p "$packagesDir/$pn/"
    (
        cd ~ || die
        mkdir -p ember-auto-build-temp
        cd ember-auto-build-temp
        if [[ -e "$repository" ]]; then
            pushd "$repository" || die
            git pull
            popd || die
        else
            git clone "https://github.com/ethus3h/$repository.git"
        fi
        pushd "$repository" || die
        hash="$(guix hash -rx .)"
        [[ -z "$hash" ]] && die "hash was empty"
        echo "$hash" > "$packagesDir/$pn/hash"

        commit="$(git rev-parse HEAD)"
        [[ -z "$commit" ]] && die "commit was empty"
        echo "$commit" > "$packagesDir/$pn/commit"

        version="$version-$commit"
        [[ -z "$version" ]] && die "version was empty"
        echo "$version" > "$packagesDir/$pn/version"
        popd || die
        while IFS= read -r line; do
            (
                # line: package name
                hash="$(<"$packagesDir/$line/hash")"
                ereplace "TEMPLATE-PLACEHOLDER-HASH:$line" "$hash" dce.scm
                commit="$(<"$packagesDir/$line/commit")"
                ereplace "TEMPLATE-PLACEHOLDER-COMMIT:$line" "$commit" dce.scm
                version="$(<"$packagesDir/$line/version")"
                ereplace "TEMPLATE-PLACEHOLDER-VERSION:$line" "$version" dce.scm
            )
        done < "$packagesDir"/packages.lst
        cd ~/guix || die
        cp ~/ember-auto-build-temp/dce.scm ./gnu/packages/dce.scm
        git add .; git commit -m "Automatic update of $pn from $repository (version $commit)"
    )
    cp ~/ember-auto-build-temp/dce.scm "$packagesDir"/dce.scm
    eogup-single
    testGuixPackage "$pn"
)

testAndGenerateGuixPackage() (
    generateGuixPackage "$pn"
    if [[ -n "$1" ]]; then
        pn="$1"
    else
        pn="$(<package-name)"
    fi
)
